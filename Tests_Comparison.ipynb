{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: Class Imbalance\n",
      "Confusion Matrix:\n",
      "[[7 0]\n",
      " [3 0]]\n",
      "Accuracy: 0.7\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Class Imbalance\n",
    "# Scenario:\n",
    "# Class imbalance occurs when the number of instances in different classes is significantly different.\n",
    "# For example, in a binary classification problem, one class has many more instances than the other.\n",
    "# Issue with Accuracy:\n",
    "#               Accuracy can be misleading in the presence of class imbalance.\n",
    "#               A classifier that predicts the majority class for all instances can have high accuracy but provides little value.\n",
    "# Advantage of F1-score:\n",
    "#               F1-score considers both precision and recall, making it a more informative metric in the presence of imbalanced classes.\n",
    "#               It is less affected by the distribution of classes.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# True labels (binary classification: 0 and 1)\n",
    "y_true = np.array([0, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "\n",
    "# Predictions (predicting the majority class for simplicity)\n",
    "y_pred_majority = np.zeros_like(y_true) # array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Metrics\n",
    "accuracy_majority = accuracy_score(y_true, y_pred_majority)\n",
    "f1_majority = f1_score(y_true, y_pred_majority)\n",
    "\n",
    "print(\"Scenario 1: Class Imbalance\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_majority))\n",
    "print(\"Accuracy:\", accuracy_majority)\n",
    "print(\"F1 Score:\", f1_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 2: Asymmetric Costs\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [5 0]]\n",
      "Accuracy: 0.5\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Asymmetric Costs:\n",
    "# Scenario:\n",
    "# In some applications, the cost of false positives (Type I errors) may be different from the cost of false negatives (Type II errors).\n",
    "# For example, in medical diagnosis, a false negative (missing a disease) might have more severe consequences than a false positive.\n",
    "# Issue with Accuracy:\n",
    "#               Accuracy treats false positives and false negatives equally, which may not reflect the true impact of different types of errors.\n",
    "# Advantage of F1-score:\n",
    "#               F1-score provides a balance between precision and recall, making it suitable for scenarios where the cost of false positives and false negatives differs.\n",
    "\n",
    "# Increase the cost of false negatives (missing the positive class)\n",
    "y_true_asymmetric = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "# Predictions (predicting the majority class for simplicity)\n",
    "y_pred_majority_asymmetric = np.zeros_like(y_true_asymmetric)\n",
    "\n",
    "# Metrics\n",
    "accuracy_majority_asymmetric = accuracy_score(y_true_asymmetric, y_pred_majority_asymmetric)\n",
    "f1_majority_asymmetric = f1_score(y_true_asymmetric, y_pred_majority_asymmetric)\n",
    "\n",
    "print(\"\\nScenario 2: Asymmetric Costs\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_asymmetric, y_pred_majority_asymmetric))\n",
    "print(\"Accuracy:\", accuracy_majority_asymmetric)\n",
    "print(\"F1 Score:\", f1_majority_asymmetric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 3: Threshold Sensitivity\n",
      "Confusion Matrix:\n",
      "[[6 1]\n",
      " [0 3]]\n",
      "Accuracy: 0.9\n",
      "F1 Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# 3. Threshold Sensitivity:\n",
    "# Scenario:\n",
    "# In some cases, you might need to adjust the classification threshold to prioritize precision or recall based on the application's requirements.\n",
    "# Issue with Accuracy:\n",
    "#               Accuracy doesn't provide insights into the classifier's behavior at different thresholds.\n",
    "# Advantage of F1-score:\n",
    "#               F1-score considers precision and recall at different thresholds, providing a more nuanced evaluation of model performance.\n",
    "\n",
    "# Use a threshold for binary classification (0.3 for demonstration purposes)\n",
    "y_pred_threshold = np.array([0, 0, 1, 0, 0, 1, 0, 0, 1, 0.3])\n",
    "\n",
    "# Binarize predictions based on the threshold\n",
    "y_pred_binary = (y_pred_threshold >= 0.3).astype(int)\n",
    "\n",
    "# Metrics\n",
    "accuracy_threshold = accuracy_score(y_true, y_pred_binary)\n",
    "f1_threshold = f1_score(y_true, y_pred_binary)\n",
    "\n",
    "print(\"\\nScenario 3: Threshold Sensitivity\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_binary))\n",
    "print(\"Accuracy:\", accuracy_threshold)\n",
    "print(\"F1 Score:\", f1_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
